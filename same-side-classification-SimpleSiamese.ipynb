{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cross_path = '../argmining19-same-side-classification/data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = '../argmining19-same-side-classification/data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, clone_model, load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score, f1_score\n",
    "def report_training_results(y_test, y_pred):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))  \n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2))  #\n",
    "    print()\n",
    "\n",
    "    print('Report:')\n",
    "    print(classification_report(y_test, y_pred))  \n",
    "    f1_dic = {}\n",
    "    \n",
    "    f1_dic['macro'] = round(f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../argmining19-same-side-classification/data/same-side-classification/cross-topic/training.csv\n",
      "../argmining19-same-side-classification/data/same-side-classification/cross-topic/test.csv\n",
      "../argmining19-same-side-classification/data/same-side-classification/within-topic/training.csv\n",
      "../argmining19-same-side-classification/data/same-side-classification/within-topic/test.csv\n"
     ]
    }
   ],
   "source": [
    "print(data_cross_path.format('training'))\n",
    "cross_train_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                quotechar='\"',\n",
    "                                quoting=csv.QUOTE_ALL,\n",
    "                                encoding='utf-8',\n",
    "                                escapechar='\\\\',\n",
    "                                doublequote=False,\n",
    "                                index_col='id')\n",
    "print(data_cross_path.format('test'))\n",
    "cross_test_df = pd.read_csv(data_cross_path.format('test'),\n",
    "                            # quotechar='\"',\n",
    "                            # quoting=csv.QUOTE_ALL,\n",
    "                            # encoding='utf-8',\n",
    "                            # escapechar='\\\\',\n",
    "                            # doublequote=False,\n",
    "                            index_col='id')\n",
    "\n",
    "print(data_within_path.format('training'))\n",
    "within_train_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                 quotechar='\"',\n",
    "                                 quoting=csv.QUOTE_ALL,\n",
    "                                 encoding='utf-8',\n",
    "                                 escapechar='\\\\',\n",
    "                                 doublequote=False,\n",
    "                                 index_col='id')\n",
    "print(data_within_path.format('test'))\n",
    "within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "                             quotechar='\"',\n",
    "                             # quoting=csv.QUOTE_ALL,\n",
    "                             # encoding='utf-8',\n",
    "                             # escapechar='\\\\',\n",
    "                             # doublequote=False,\n",
    "                             index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if title.find('abortion') > -1 :\n",
    "        row['tag'] = 'abortion'\n",
    "    elif title.find('gay marriage') > -1 :\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "cross_train_df = cross_train_df.apply(add_tag, axis=1)\n",
    "# cross_dev_df = cross_dev_df.apply(add_tag, axis=1)\n",
    "cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "within_train_df = within_train_df.apply(add_tag, axis=1)\n",
    "# within_dev_df = within_dev_df.apply(add_tag, axis=1)\n",
    "within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument1</th>\n",
       "      <th>argument1_id</th>\n",
       "      <th>argument2</th>\n",
       "      <th>argument2_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>is_same_side</th>\n",
       "      <th>topic</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there are two reasons why this debate should g...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>i will give my opponent a chance to respond.</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>True</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are two reasons why this debate should g...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>in this debate, there are a few factors that m...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>True</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument1  \\\n",
       "id                                                      \n",
       "0   there are two reasons why this debate should g...   \n",
       "1   there are two reasons why this debate should g...   \n",
       "\n",
       "                               argument1_id  \\\n",
       "id                                            \n",
       "0   100c174f-2019-04-18T17:33:51Z-00000-000   \n",
       "1   100c174f-2019-04-18T17:33:51Z-00000-000   \n",
       "\n",
       "                                            argument2  \\\n",
       "id                                                      \n",
       "0        i will give my opponent a chance to respond.   \n",
       "1   in this debate, there are a few factors that m...   \n",
       "\n",
       "                               argument2_id                      debate_id  \\\n",
       "id                                                                           \n",
       "0   100c174f-2019-04-18T17:33:51Z-00000-000  100c174f-2019-04-18T17:33:51Z   \n",
       "1   100c174f-2019-04-18T17:33:51Z-00000-000  100c174f-2019-04-18T17:33:51Z   \n",
       "\n",
       "    is_same_side                                       topic       tag  \n",
       "id                                                                      \n",
       "0           True  abortion should be illegal with exceptions  abortion  \n",
       "1           True  abortion should be illegal with exceptions  abortion  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument1</th>\n",
       "      <th>argument2</th>\n",
       "      <th>topic</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i would like to start off by thanking my oppon...</td>\n",
       "      <td>i was hoping that since this member took the t...</td>\n",
       "      <td>gay marriage is wrong</td>\n",
       "      <td>gay marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i would like to start off by thanking my oppon...</td>\n",
       "      <td>hello, i am new to this website and usually de...</td>\n",
       "      <td>gay marriage is wrong</td>\n",
       "      <td>gay marriage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument1  \\\n",
       "id                                                      \n",
       "0   i would like to start off by thanking my oppon...   \n",
       "1   i would like to start off by thanking my oppon...   \n",
       "\n",
       "                                            argument2                  topic  \\\n",
       "id                                                                             \n",
       "0   i was hoping that since this member took the t...  gay marriage is wrong   \n",
       "1   hello, i am new to this website and usually de...  gay marriage is wrong   \n",
       "\n",
       "             tag  \n",
       "id                \n",
       "0   gay marriage  \n",
       "1   gay marriage  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument1</th>\n",
       "      <th>argument1_id</th>\n",
       "      <th>argument2</th>\n",
       "      <th>argument2_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>is_same_side</th>\n",
       "      <th>topic</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85249</th>\n",
       "      <td>gay marriage devalues marriage, frequency of o...</td>\n",
       "      <td>d2f4b1cd-2019-04-17T11:47:27Z-00063-000</td>\n",
       "      <td>being unaccustomed to gay marriage is no argument</td>\n",
       "      <td>d2f4b1cd-2019-04-17T11:47:27Z-00063-000</td>\n",
       "      <td>d2f4b1cd-2019-04-17T11:47:27Z</td>\n",
       "      <td>False</td>\n",
       "      <td>gay marriage, debate on same sex marriage</td>\n",
       "      <td>gay marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>accepted. pro may extend their arguments to th...</td>\n",
       "      <td>2a0d32eb-2019-04-18T11:46:44Z-00004-000</td>\n",
       "      <td>i\"m pro-life. just think about it, your murder...</td>\n",
       "      <td>2a0d32eb-2019-04-18T11:46:44Z-00004-000</td>\n",
       "      <td>2a0d32eb-2019-04-18T11:46:44Z</td>\n",
       "      <td>False</td>\n",
       "      <td>abortion (pro life)</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               argument1  \\\n",
       "id                                                         \n",
       "85249  gay marriage devalues marriage, frequency of o...   \n",
       "2607   accepted. pro may extend their arguments to th...   \n",
       "\n",
       "                                  argument1_id  \\\n",
       "id                                               \n",
       "85249  d2f4b1cd-2019-04-17T11:47:27Z-00063-000   \n",
       "2607   2a0d32eb-2019-04-18T11:46:44Z-00004-000   \n",
       "\n",
       "                                               argument2  \\\n",
       "id                                                         \n",
       "85249  being unaccustomed to gay marriage is no argument   \n",
       "2607   i\"m pro-life. just think about it, your murder...   \n",
       "\n",
       "                                  argument2_id                      debate_id  \\\n",
       "id                                                                              \n",
       "85249  d2f4b1cd-2019-04-17T11:47:27Z-00063-000  d2f4b1cd-2019-04-17T11:47:27Z   \n",
       "2607   2a0d32eb-2019-04-18T11:46:44Z-00004-000  2a0d32eb-2019-04-18T11:46:44Z   \n",
       "\n",
       "       is_same_side                                      topic           tag  \n",
       "id                                                                            \n",
       "85249         False  gay marriage, debate on same sex marriage  gay marriage  \n",
       "2607          False                        abortion (pro life)      abortion  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument1</th>\n",
       "      <th>argument2</th>\n",
       "      <th>topic</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i would like to start off by thanking my oppon...</td>\n",
       "      <td>hello, i am new to this website and usually de...</td>\n",
       "      <td>gay marriage is wrong</td>\n",
       "      <td>gay marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>don't judge a book by its cover. you neef obje...</td>\n",
       "      <td>the bible has multiple versions so you can't s...</td>\n",
       "      <td>gay marriage is wrong</td>\n",
       "      <td>gay marriage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument1  \\\n",
       "id                                                      \n",
       "11  i would like to start off by thanking my oppon...   \n",
       "20  don't judge a book by its cover. you neef obje...   \n",
       "\n",
       "                                            argument2                  topic  \\\n",
       "id                                                                             \n",
       "11  hello, i am new to this website and usually de...  gay marriage is wrong   \n",
       "20  the bible has multiple versions so you can't s...  gay marriage is wrong   \n",
       "\n",
       "             tag  \n",
       "id                \n",
       "11  gay marriage  \n",
       "20  gay marriage  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTify training and test data\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = get_train_test_sets(within_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rioreiser/miniconda3/envs/sameside/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=256\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "a1 = bc.encode(X_train.argument1.tolist())\n",
    "a2 = bc.encode(X_train.argument2.tolist())\n",
    "train_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(train_embedded_pairs, open(\"BERT_pairs_train.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = bc.encode(X_dev.argument1.tolist())\n",
    "a2 = bc.encode(X_dev.argument2.tolist())\n",
    "dev_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(dev_embedded_pairs, open(\"BERT_pairs_dev.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = bc.encode(within_test_df.argument1.tolist())\n",
    "a2 = bc.encode(within_test_df.argument2.tolist())\n",
    "test_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(test_embedded_pairs, open(\"BERT_pairs_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embs = pickle.load(open(\"BERT_pairs_train.pkl\",\"rb\"))\n",
    "dev_embs = pickle.load(open(\"BERT_pairs_dev.pkl\",\"rb\"))\n",
    "test_embs = pickle.load(open(\"BERT_pairs_test.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57467, 1024)\n"
     ]
    }
   ],
   "source": [
    "t1, t2 = zip(*training_embs)\n",
    "train_args1 = np.array(t1)\n",
    "train_args2 = np.array(t2)\n",
    "print(train_args1.shape)\n",
    "train_tags = to_categorical([0 if t=='abortion' else 1 for t in within_train_df.tag.tolist()], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4498, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2 = zip(*dev_embs)\n",
    "dev_args1 = np.array(t1)\n",
    "dev_args2 = np.array(t2)\n",
    "dev_tags = to_categorical([0 if t=='abortion' else 1 for t in within_dev_df.tag.tolist()], num_classes=2)\n",
    "dev_args1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1938, 1024)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2 = zip(*test_embs)\n",
    "test_args1 = np.array(t1)\n",
    "test_args2 = np.array(t2)\n",
    "test_tags = to_categorical([0 if t=='abortion' else 1 for t in within_test_df.tag.tolist()], num_classes=2)\n",
    "test_args1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = [1 if t else 0 for t in within_train_df.is_same_side]\n",
    "dev_output = [1 if t else 0 for t in within_dev_df.is_same_side]\n",
    "test_output = [1 if t else 0 for t in within_test_df.is_same_side]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/home/gwiedemann/miniconda3/envs/nnnlp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = t1[0].shape\n",
    "\n",
    "layer_input_1 = Input(shape = embedding_dims, name = 'input_1')\n",
    "layer_input_2 = Input(shape = embedding_dims, name = 'input_2')\n",
    "layer_input_3 = Input(shape = (2,), name = 'input_3')\n",
    "\n",
    "encoder_1 = Dense(embedding_dims[0], activation='relu')(layer_input_1)\n",
    "encoder_2 = Dense(embedding_dims[0], activation='relu')(layer_input_2)\n",
    "\n",
    "# combined_diff = subtract([encoder_1, encoder_2])\n",
    "# combined_mult = multiply([encoder_1, encoder_2])\n",
    "combined_dot = dot([encoder_1, encoder_2], axes=-1, normalize=False)\n",
    "combined_all = concatenate([encoder_1, encoder_2, combined_dot, layer_input_3])\n",
    "\n",
    "features_pred = Dense(300, activation='relu')(combined_all)\n",
    "# features_norm = BatchNormalization()(features_pred)\n",
    "# features_acti = LeakyReLU()(features_pred)\n",
    "# features_drop = Dropout(rate=0.5)(features_acti)\n",
    "layer_prediction = Dense(1, activation='sigmoid')(features_pred)\n",
    "\n",
    "model = Model([layer_input_1, layer_input_2, layer_input_3], layer_prediction)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"argmining19_simple-nn-model.hdf5\", monitor='val_acc', \n",
    "                             verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/home/gwiedemann/miniconda3/envs/nnnlp/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 57467 samples, validate on 4498 samples\n",
      "Epoch 1/50\n",
      "57467/57467 [==============================] - 23s 404us/step - loss: 0.3694 - acc: 0.7852 - val_loss: 0.2862 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81948, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 2/50\n",
      "57467/57467 [==============================] - 13s 224us/step - loss: 0.2809 - acc: 0.8288 - val_loss: 0.2739 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.81948 to 0.82970, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 3/50\n",
      "57467/57467 [==============================] - 13s 224us/step - loss: 0.2654 - acc: 0.8411 - val_loss: 0.2792 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82970\n",
      "Epoch 4/50\n",
      "57467/57467 [==============================] - 13s 224us/step - loss: 0.2526 - acc: 0.8506 - val_loss: 0.2619 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.82970 to 0.84126, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 5/50\n",
      "57467/57467 [==============================] - 13s 227us/step - loss: 0.2457 - acc: 0.8548 - val_loss: 0.2613 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.84126\n",
      "Epoch 6/50\n",
      "57467/57467 [==============================] - 13s 225us/step - loss: 0.2397 - acc: 0.8605 - val_loss: 0.2602 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.84126 to 0.84371, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 7/50\n",
      "57467/57467 [==============================] - 13s 223us/step - loss: 0.2325 - acc: 0.8663 - val_loss: 0.2588 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84371 to 0.84882, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 8/50\n",
      "57467/57467 [==============================] - 13s 219us/step - loss: 0.2253 - acc: 0.8704 - val_loss: 0.2663 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.84882\n",
      "Epoch 9/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.2201 - acc: 0.8775 - val_loss: 0.2727 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.84882 to 0.85149, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 10/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.2137 - acc: 0.8826 - val_loss: 0.2765 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.85149 to 0.85305, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 11/50\n",
      "57467/57467 [==============================] - 13s 220us/step - loss: 0.2056 - acc: 0.8872 - val_loss: 0.2742 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85305\n",
      "Epoch 12/50\n",
      "57467/57467 [==============================] - 13s 225us/step - loss: 0.1964 - acc: 0.8934 - val_loss: 0.2736 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85305\n",
      "Epoch 13/50\n",
      "57467/57467 [==============================] - 13s 220us/step - loss: 0.1917 - acc: 0.8964 - val_loss: 0.2766 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.85305 to 0.85349, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 14/50\n",
      "57467/57467 [==============================] - 13s 223us/step - loss: 0.1843 - acc: 0.9010 - val_loss: 0.2903 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85349\n",
      "Epoch 15/50\n",
      "57467/57467 [==============================] - 13s 222us/step - loss: 0.1773 - acc: 0.9056 - val_loss: 0.3196 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85349\n",
      "Epoch 16/50\n",
      "57467/57467 [==============================] - 13s 222us/step - loss: 0.1724 - acc: 0.9103 - val_loss: 0.3285 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.85349 to 0.85505, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 17/50\n",
      "57467/57467 [==============================] - 13s 219us/step - loss: 0.1640 - acc: 0.9150 - val_loss: 0.3510 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85505\n",
      "Epoch 18/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.1619 - acc: 0.9177 - val_loss: 0.3306 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85505\n",
      "Epoch 19/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.1512 - acc: 0.9236 - val_loss: 0.3505 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.85505 to 0.85705, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 20/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.1463 - acc: 0.9258 - val_loss: 0.3649 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85705\n",
      "Epoch 21/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.1410 - acc: 0.9300 - val_loss: 0.3625 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85705\n",
      "Epoch 22/50\n",
      "57467/57467 [==============================] - 12s 217us/step - loss: 0.1370 - acc: 0.9326 - val_loss: 0.3779 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85705 to 0.86105, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 23/50\n",
      "57467/57467 [==============================] - 13s 224us/step - loss: 0.1327 - acc: 0.9363 - val_loss: 0.4141 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86105\n",
      "Epoch 24/50\n",
      "57467/57467 [==============================] - 13s 222us/step - loss: 0.1273 - acc: 0.9386 - val_loss: 0.3788 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86105\n",
      "Epoch 25/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.1226 - acc: 0.9414 - val_loss: 0.4038 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86105\n",
      "Epoch 26/50\n",
      "57467/57467 [==============================] - 12s 214us/step - loss: 0.1155 - acc: 0.9444 - val_loss: 0.4782 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86105\n",
      "Epoch 27/50\n",
      "57467/57467 [==============================] - 12s 217us/step - loss: 0.1218 - acc: 0.9458 - val_loss: 0.4387 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.86105 to 0.86149, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 28/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.1091 - acc: 0.9481 - val_loss: 0.4897 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.86149\n",
      "Epoch 29/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.1021 - acc: 0.9520 - val_loss: 0.5062 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.86149 to 0.86238, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 30/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.1024 - acc: 0.9525 - val_loss: 0.4883 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86238\n",
      "Epoch 31/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0993 - acc: 0.9543 - val_loss: 0.5574 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.86238 to 0.86438, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 32/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.0932 - acc: 0.9567 - val_loss: 0.5361 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.86438\n",
      "Epoch 33/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.0916 - acc: 0.9575 - val_loss: 0.5247 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.86438\n",
      "Epoch 34/50\n",
      "57467/57467 [==============================] - 13s 219us/step - loss: 0.0885 - acc: 0.9601 - val_loss: 0.5622 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.86438\n",
      "Epoch 35/50\n",
      "57467/57467 [==============================] - 13s 221us/step - loss: 0.0854 - acc: 0.9619 - val_loss: 0.5552 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.86438 to 0.86572, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 36/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0835 - acc: 0.9629 - val_loss: 0.5574 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.86572\n",
      "Epoch 37/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0825 - acc: 0.9634 - val_loss: 0.6430 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.86572\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0748 - acc: 0.9656 - val_loss: 0.5577 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.86572 to 0.86616, saving model to argmining19_simple-nn-model.hdf5\n",
      "Epoch 39/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0773 - acc: 0.9663 - val_loss: 0.5868 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86616\n",
      "Epoch 40/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0791 - acc: 0.9671 - val_loss: 0.6109 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.86616\n",
      "Epoch 41/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.0661 - acc: 0.9700 - val_loss: 0.6506 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.86616\n",
      "Epoch 42/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0666 - acc: 0.9701 - val_loss: 0.6603 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.86616\n",
      "Epoch 43/50\n",
      "57467/57467 [==============================] - 12s 217us/step - loss: 0.0740 - acc: 0.9691 - val_loss: 0.7045 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.86616\n",
      "Epoch 44/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0640 - acc: 0.9704 - val_loss: 0.6719 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86616\n",
      "Epoch 45/50\n",
      "57467/57467 [==============================] - 13s 219us/step - loss: 0.0660 - acc: 0.9718 - val_loss: 0.6959 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86616\n",
      "Epoch 46/50\n",
      "57467/57467 [==============================] - 13s 221us/step - loss: 0.0595 - acc: 0.9737 - val_loss: 0.6957 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86616\n",
      "Epoch 47/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.0641 - acc: 0.9727 - val_loss: 0.7272 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.86616\n",
      "Epoch 48/50\n",
      "57467/57467 [==============================] - 12s 217us/step - loss: 0.0603 - acc: 0.9740 - val_loss: 0.7407 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86616\n",
      "Epoch 49/50\n",
      "57467/57467 [==============================] - 12s 215us/step - loss: 0.0561 - acc: 0.9749 - val_loss: 0.7539 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.86616\n",
      "Epoch 50/50\n",
      "57467/57467 [==============================] - 12s 216us/step - loss: 0.0578 - acc: 0.9755 - val_loss: 0.7326 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67b9754978>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_args1, train_args2, train_tags], train_output, batch_size=32, epochs=50, \n",
    "          validation_data=([dev_args1, dev_args2, dev_tags], dev_output), verbose=True,\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"argmining19_simple-nn-model.hdf5\")\n",
    "dev_output_pred = np.round(model.predict([dev_args1, dev_args2, dev_tags]))\n",
    "test_output_pred = np.round(model.predict([test_args1, test_args2, test_tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev set performances\n",
    "# --------------------\n",
    "\n",
    "# concatenate([encoder_1, encoder_2, combined_diff, combined_dot])\n",
    "# Epoch 17/20\n",
    "# 57512/57512 [==============================] - 12s 207us/step - loss: 0.1906 - acc: 0.8963 - val_loss: 0.3081 - val_acc: 0.8512\n",
    "\n",
    "# combined_all = concatenate([encoder_1, encoder_2])\n",
    "# Epoch 17/20\n",
    "# 57512/57512 [==============================] - 11s 191us/step - loss: 0.1767 - acc: 0.9052 - val_loss: 0.3394 - val_acc: 0.8514\n",
    "\n",
    "# combined_all = concatenate([encoder_1, encoder_2, combined_dot])\n",
    "# Epoch 17/20\n",
    "# 57512/57512 [==============================] - 12s 205us/step - loss: 0.1726 - acc: 0.9115 - val_loss: 0.3162 - val_acc: 0.8559\n",
    "\n",
    "# + dropout 0.1: hülft nicht viel... bzw. nichts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1752  306]\n",
      " [ 296 2144]]\n",
      "\n",
      "Accuracy:  0.87\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.85      0.85      2058\n",
      "          1       0.88      0.88      0.88      2440\n",
      "\n",
      "avg / total       0.87      0.87      0.87      4498\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'macro': 0.87, 'micro': 0.87}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev set performance\n",
    "report_training_results(dev_output, dev_output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[794 114]\n",
      " [128 902]]\n",
      "\n",
      "Accuracy:  0.88\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.87      0.87       908\n",
      "          1       0.89      0.88      0.88      1030\n",
      "\n",
      "avg / total       0.88      0.88      0.88      1938\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'macro': 0.87, 'micro': 0.88}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set performance\n",
    "report_training_results(test_output, test_output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         1049600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2051)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          615600      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            301         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,715,101\n",
      "Trainable params: 2,715,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_test_all =  pd.read_csv(\"data/same-side-classification/within-topic/test.csv\", index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument1</th>\n",
       "      <th>argument2</th>\n",
       "      <th>topic</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95327</th>\n",
       "      <td>(resolved)on balance:middleclassandrichwomenwh...</td>\n",
       "      <td>(resolved)on balance:middleclassandrichwomenwh...</td>\n",
       "      <td>(resolved)on balance:middleclassandrichwomenwh...</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95345</th>\n",
       "      <td>(resolved)on balance:middleclassandrichwomenwh...</td>\n",
       "      <td>(resolved)on balance:middleclassandrichwomenwh...</td>\n",
       "      <td>(resolved)on balance:middleclassandrichwomenwh...</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95364</th>\n",
       "      <td>gay marriage should be legalized in america i'...</td>\n",
       "      <td>gay marriage should be legalized in america i ...</td>\n",
       "      <td>gay marriage should be legalized in america</td>\n",
       "      <td>gay marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95369</th>\n",
       "      <td>every human being has rights, even if they can...</td>\n",
       "      <td>first, i must say that i do not under any circ...</td>\n",
       "      <td>live birth abortion should stay illegal.</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95370</th>\n",
       "      <td>yes, but the baby is still not alive, it is st...</td>\n",
       "      <td>until a baby is born naturally, it is not trul...</td>\n",
       "      <td>live birth abortion should stay illegal.</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               argument1  \\\n",
       "id                                                         \n",
       "95327  (resolved)on balance:middleclassandrichwomenwh...   \n",
       "95345  (resolved)on balance:middleclassandrichwomenwh...   \n",
       "95364  gay marriage should be legalized in america i'...   \n",
       "95369  every human being has rights, even if they can...   \n",
       "95370  yes, but the baby is still not alive, it is st...   \n",
       "\n",
       "                                               argument2  \\\n",
       "id                                                         \n",
       "95327  (resolved)on balance:middleclassandrichwomenwh...   \n",
       "95345  (resolved)on balance:middleclassandrichwomenwh...   \n",
       "95364  gay marriage should be legalized in america i ...   \n",
       "95369  first, i must say that i do not under any circ...   \n",
       "95370  until a baby is born naturally, it is not trul...   \n",
       "\n",
       "                                                   topic           tag  \n",
       "id                                                                      \n",
       "95327  (resolved)on balance:middleclassandrichwomenwh...      abortion  \n",
       "95345  (resolved)on balance:middleclassandrichwomenwh...      abortion  \n",
       "95364        gay marriage should be legalized in america  gay marriage  \n",
       "95369           live birth abortion should stay illegal.      abortion  \n",
       "95370           live birth abortion should stay illegal.      abortion  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_test_all = within_test_all.apply(add_tag, axis=1)\n",
    "within_test_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3c130ca99d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithin_test_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margument1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithin_test_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margument2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtestall_embedded_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestall_embedded_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT_pairs_testall.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bc' is not defined"
     ]
    }
   ],
   "source": [
    "a1 = bc.encode(within_test_all.argument1.tolist())\n",
    "a2 = bc.encode(within_test_all.argument2.tolist())\n",
    "testall_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(testall_embedded_pairs, open(\"BERT_pairs_testall.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testall_embs = pickle.load(open(\"BERT_pairs_testall.pkl\",\"rb\"))\n",
    "t1, t2 = zip(*testall_embs)\n",
    "testall_args1 = np.array(t1)\n",
    "testall_args2 = np.array(t2)\n",
    "testall_args1.shape\n",
    "testall_tags = to_categorical([0 if t=='abortion' else 1 for t in within_train_df.tag.tolist()], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"argmining19_simple-nn-model.hdf5\")\n",
    "testall_output_pred = model.predict([testall_args1, testall_args2, testall_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk\n",
    "with open(\"predicted_labels_simple-nn_within.csv\", \"w\") as f:\n",
    "    for i, a in enumerate(testall_output_pred):\n",
    "        f.write(str(within_test_all.index.values[i]) + \",\" + str(a[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67102396],\n",
       "       [0.6054134 ],\n",
       "       [0.9379481 ],\n",
       "       ...,\n",
       "       [0.6328175 ],\n",
       "       [0.63011324],\n",
       "       [0.02002504]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testall_output_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_dev_df = within_train_df[within_train_df.tag == \"gay marriage\"][:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cross_dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/gwiedemann/miniconda3/envs/nnnlp/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=256\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "# dev set\n",
    "a1 = bc.encode(cross_dev_df.argument1.tolist())\n",
    "a2 = bc.encode(cross_dev_df.argument2.tolist())\n",
    "dev_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(dev_embedded_pairs, open(\"BERT_pairs_cross_dev.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument1</th>\n",
       "      <th>argument1_id</th>\n",
       "      <th>argument2</th>\n",
       "      <th>argument2_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>is_same_side</th>\n",
       "      <th>topic</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there are two reasons why this debate should g...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>i will give my opponent a chance to respond.</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>True</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are two reasons why this debate should g...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>in this debate, there are a few factors that m...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>True</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first i want to thank my opponent for letting ...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00001-000</td>\n",
       "      <td>this is my first debate so please just bare wi...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00001-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>True</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i will give my opponent a chance to respond.</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00002-000</td>\n",
       "      <td>in this debate, there are a few factors that m...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00002-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>True</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there are two reasons why this debate should g...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>first i want to thank my opponent for letting ...</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z-00000-000</td>\n",
       "      <td>100c174f-2019-04-18T17:33:51Z</td>\n",
       "      <td>False</td>\n",
       "      <td>abortion should be illegal with exceptions</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument1  \\\n",
       "id                                                      \n",
       "0   there are two reasons why this debate should g...   \n",
       "1   there are two reasons why this debate should g...   \n",
       "2   first i want to thank my opponent for letting ...   \n",
       "3        i will give my opponent a chance to respond.   \n",
       "4   there are two reasons why this debate should g...   \n",
       "\n",
       "                               argument1_id  \\\n",
       "id                                            \n",
       "0   100c174f-2019-04-18T17:33:51Z-00000-000   \n",
       "1   100c174f-2019-04-18T17:33:51Z-00000-000   \n",
       "2   100c174f-2019-04-18T17:33:51Z-00001-000   \n",
       "3   100c174f-2019-04-18T17:33:51Z-00002-000   \n",
       "4   100c174f-2019-04-18T17:33:51Z-00000-000   \n",
       "\n",
       "                                            argument2  \\\n",
       "id                                                      \n",
       "0        i will give my opponent a chance to respond.   \n",
       "1   in this debate, there are a few factors that m...   \n",
       "2   this is my first debate so please just bare wi...   \n",
       "3   in this debate, there are a few factors that m...   \n",
       "4   first i want to thank my opponent for letting ...   \n",
       "\n",
       "                               argument2_id                      debate_id  \\\n",
       "id                                                                           \n",
       "0   100c174f-2019-04-18T17:33:51Z-00000-000  100c174f-2019-04-18T17:33:51Z   \n",
       "1   100c174f-2019-04-18T17:33:51Z-00000-000  100c174f-2019-04-18T17:33:51Z   \n",
       "2   100c174f-2019-04-18T17:33:51Z-00001-000  100c174f-2019-04-18T17:33:51Z   \n",
       "3   100c174f-2019-04-18T17:33:51Z-00002-000  100c174f-2019-04-18T17:33:51Z   \n",
       "4   100c174f-2019-04-18T17:33:51Z-00000-000  100c174f-2019-04-18T17:33:51Z   \n",
       "\n",
       "    is_same_side                                       topic       tag  \n",
       "id                                                                      \n",
       "0           True  abortion should be illegal with exceptions  abortion  \n",
       "1           True  abortion should be illegal with exceptions  abortion  \n",
       "2           True  abortion should be illegal with exceptions  abortion  \n",
       "3           True  abortion should be illegal with exceptions  abortion  \n",
       "4          False  abortion should be illegal with exceptions  abortion  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/gwiedemann/miniconda3/envs/nnnlp/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=256\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "# BERTify training and test data\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "# train set\n",
    "a1 = bc.encode(cross_train_df.argument1.tolist())\n",
    "a2 = bc.encode(cross_train_df.argument2.tolist())\n",
    "train_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(train_embedded_pairs, open(\"BERT_pairs_cross_train.pkl\", \"wb\"))\n",
    "# dev set\n",
    "a1 = bc.encode(cross_dev_df.argument1.tolist())\n",
    "a2 = bc.encode(cross_dev_df.argument2.tolist())\n",
    "dev_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(dev_embedded_pairs, open(\"BERT_pairs_cross_dev.pkl\", \"wb\"))\n",
    "# test set\n",
    "a1 = bc.encode(cross_test_df.argument1.tolist())\n",
    "a2 = bc.encode(cross_test_df.argument2.tolist())\n",
    "test_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(test_embedded_pairs, open(\"BERT_pairs_cross_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embs = pickle.load(open(\"BERT_pairs_cross_train.pkl\",\"rb\"))\n",
    "dev_embs = pickle.load(open(\"BERT_pairs_cross_dev.pkl\",\"rb\"))\n",
    "test_embs = pickle.load(open(\"BERT_pairs_cross_test.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "t1, t2 = zip(*training_embs)\n",
    "train_args1 = np.array(t1)\n",
    "train_args2 = np.array(t2)\n",
    "# dev set\n",
    "t1, t2 = zip(*dev_embs)\n",
    "dev_args1 = np.array(t1)\n",
    "dev_args2 = np.array(t2)\n",
    "# test set\n",
    "t1, t2 = zip(*test_embs)\n",
    "test_args1 = np.array(t1)\n",
    "test_args2 = np.array(t2)\n",
    "# outputs\n",
    "train_output = [1 if t else 0 for t in cross_train_df.is_same_side]\n",
    "dev_output = [1 if t else 0 for t in cross_dev_df.is_same_side]\n",
    "test_output = [1 if t else 0 for t in cross_test_df.is_same_side]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "embedding_dims = t1[0].shape\n",
    "\n",
    "layer_input_1 = Input(shape = embedding_dims, name = 'input_1')\n",
    "layer_input_2 = Input(shape = embedding_dims, name = 'input_2')\n",
    "\n",
    "encoder_1 = Dense(embedding_dims[0], activation='relu')(layer_input_1)\n",
    "encoder_2 = Dense(embedding_dims[0], activation='relu')(layer_input_2)\n",
    "combined_dot = dot([encoder_1, encoder_2], axes=-1, normalize=False)\n",
    "combined_all = concatenate([encoder_1, encoder_2, combined_dot])\n",
    "\n",
    "features_pred = Dense(300, activation='relu')(combined_all)\n",
    "layer_prediction = Dense(1, activation='sigmoid')(features_pred)\n",
    "\n",
    "model = Model([layer_input_1, layer_input_2], layer_prediction)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54947 samples, validate on 3000 samples\n",
      "Epoch 1/30\n",
      "54947/54947 [==============================] - 15s 275us/step - loss: 0.3754 - acc: 0.7774 - val_loss: 1.1004 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60433, saving model to argmining19_simple-nn-model_cross.hdf5\n",
      "Epoch 2/30\n",
      "54947/54947 [==============================] - 12s 217us/step - loss: 0.3018 - acc: 0.8125 - val_loss: 1.1684 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.60433\n",
      "Epoch 3/30\n",
      "54947/54947 [==============================] - 12s 212us/step - loss: 0.2849 - acc: 0.8248 - val_loss: 1.8151 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60433 to 0.60567, saving model to argmining19_simple-nn-model_cross.hdf5\n",
      "Epoch 4/30\n",
      "54947/54947 [==============================] - 12s 213us/step - loss: 0.2708 - acc: 0.8344 - val_loss: 1.6992 - val_acc: 0.5933\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60567\n",
      "Epoch 5/30\n",
      "54947/54947 [==============================] - 12s 211us/step - loss: 0.2611 - acc: 0.8420 - val_loss: 1.6905 - val_acc: 0.5790\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.60567\n",
      "Epoch 6/30\n",
      "54947/54947 [==============================] - 12s 211us/step - loss: 0.2540 - acc: 0.8491 - val_loss: 1.9085 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.60567 to 0.60667, saving model to argmining19_simple-nn-model_cross.hdf5\n",
      "Epoch 7/30\n",
      "54947/54947 [==============================] - 12s 213us/step - loss: 0.2474 - acc: 0.8537 - val_loss: 1.9167 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.60667 to 0.60833, saving model to argmining19_simple-nn-model_cross.hdf5\n",
      "Epoch 8/30\n",
      "54947/54947 [==============================] - 12s 217us/step - loss: 0.2392 - acc: 0.8611 - val_loss: 2.2926 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60833\n",
      "Epoch 9/30\n",
      "54947/54947 [==============================] - 12s 217us/step - loss: 0.2299 - acc: 0.8692 - val_loss: 2.0929 - val_acc: 0.5937\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60833\n",
      "Epoch 10/30\n",
      "54947/54947 [==============================] - 12s 216us/step - loss: 0.2221 - acc: 0.8757 - val_loss: 2.2617 - val_acc: 0.5967\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60833\n",
      "Epoch 11/30\n",
      "54947/54947 [==============================] - 12s 219us/step - loss: 0.2124 - acc: 0.8809 - val_loss: 2.5313 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60833\n",
      "Epoch 12/30\n",
      "54947/54947 [==============================] - 12s 214us/step - loss: 0.2057 - acc: 0.8889 - val_loss: 2.3829 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.60833\n",
      "Epoch 13/30\n",
      "54947/54947 [==============================] - 12s 218us/step - loss: 0.1947 - acc: 0.8937 - val_loss: 2.9677 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.60833\n",
      "Epoch 14/30\n",
      "54947/54947 [==============================] - 12s 213us/step - loss: 0.1891 - acc: 0.8982 - val_loss: 2.7678 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.60833\n",
      "Epoch 15/30\n",
      "54947/54947 [==============================] - 12s 212us/step - loss: 0.1784 - acc: 0.9045 - val_loss: 2.7547 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60833\n",
      "Epoch 16/30\n",
      "54947/54947 [==============================] - 12s 210us/step - loss: 0.1735 - acc: 0.9095 - val_loss: 2.9664 - val_acc: 0.5863\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.60833\n",
      "Epoch 17/30\n",
      "54947/54947 [==============================] - 12s 212us/step - loss: 0.1693 - acc: 0.9122 - val_loss: 2.8665 - val_acc: 0.5967\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.60833\n",
      "Epoch 18/30\n",
      "54947/54947 [==============================] - 12s 211us/step - loss: 0.1565 - acc: 0.9200 - val_loss: 3.0456 - val_acc: 0.5980\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.60833\n",
      "Epoch 19/30\n",
      "54947/54947 [==============================] - 12s 210us/step - loss: 0.1532 - acc: 0.9223 - val_loss: 2.9573 - val_acc: 0.5867\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.60833\n",
      "Epoch 20/30\n",
      "54947/54947 [==============================] - 12s 213us/step - loss: 0.1452 - acc: 0.9274 - val_loss: 3.2567 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.60833\n",
      "Epoch 21/30\n",
      "54947/54947 [==============================] - 12s 221us/step - loss: 0.1389 - acc: 0.9312 - val_loss: 3.1779 - val_acc: 0.5953\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.60833\n",
      "Epoch 22/30\n",
      "54947/54947 [==============================] - 13s 245us/step - loss: 0.1329 - acc: 0.9340 - val_loss: 3.4378 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.60833\n",
      "Epoch 23/30\n",
      "54947/54947 [==============================] - 12s 216us/step - loss: 0.1301 - acc: 0.9381 - val_loss: 3.1643 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.60833\n",
      "Epoch 24/30\n",
      "54947/54947 [==============================] - 12s 213us/step - loss: 0.1207 - acc: 0.9414 - val_loss: 3.5160 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.60833\n",
      "Epoch 25/30\n",
      "54947/54947 [==============================] - 12s 216us/step - loss: 0.1153 - acc: 0.9448 - val_loss: 3.6038 - val_acc: 0.5893\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.60833\n",
      "Epoch 26/30\n",
      "54947/54947 [==============================] - 12s 212us/step - loss: 0.1183 - acc: 0.9447 - val_loss: 3.5143 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.60833\n",
      "Epoch 27/30\n",
      "54947/54947 [==============================] - 12s 214us/step - loss: 0.1054 - acc: 0.9496 - val_loss: 3.5343 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.60833\n",
      "Epoch 28/30\n",
      "54947/54947 [==============================] - 12s 215us/step - loss: 0.1008 - acc: 0.9520 - val_loss: 3.5969 - val_acc: 0.5723\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.60833\n",
      "Epoch 29/30\n",
      "54947/54947 [==============================] - 12s 213us/step - loss: 0.1015 - acc: 0.9522 - val_loss: 3.5594 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.60833\n",
      "Epoch 30/30\n",
      "54947/54947 [==============================] - 12s 211us/step - loss: 0.0962 - acc: 0.9548 - val_loss: 3.5704 - val_acc: 0.5873\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.60833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67b7a2ff98>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"argmining19_simple-nn-model_cross.hdf5\", monitor='val_acc', \n",
    "                             verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "model.fit([train_args1, train_args2], train_output, batch_size=32, epochs=30, \n",
    "          validation_data=([dev_args1, dev_args2], dev_output), verbose=True,\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"argmining19_simple-nn-model_cross.hdf5\")\n",
    "dev_output_pred = np.round(model.predict([dev_args1, dev_args2]))\n",
    "test_output_pred = np.round(model.predict([test_args1, test_args2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 718  565]\n",
      " [ 610 1107]]\n",
      "\n",
      "Accuracy:  0.61\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.56      0.55      1283\n",
      "          1       0.66      0.64      0.65      1717\n",
      "\n",
      "avg / total       0.61      0.61      0.61      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'macro': 0.6, 'micro': 0.61}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev set performance\n",
    "report_training_results(dev_output, dev_output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 935  279]\n",
      " [ 139 1088]]\n",
      "\n",
      "Accuracy:  0.83\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.77      0.82      1214\n",
      "          1       0.80      0.89      0.84      1227\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2441\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'macro': 0.83, 'micro': 0.83}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set performance\n",
    "report_training_results(test_output, test_output_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_test_all =  pd.read_csv(\"data/same-side-classification/cross-topic/test.csv\", index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/gwiedemann/miniconda3/envs/nnnlp/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=256\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "a1 = bc.encode(cross_test_all.argument1.tolist())\n",
    "a2 = bc.encode(cross_test_all.argument2.tolist())\n",
    "testall_embedded_pairs = zip(a1, a2)\n",
    "pickle.dump(testall_embedded_pairs, open(\"BERT_pairs_testall_cross.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6163, 1024)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testall_embs = pickle.load(open(\"BERT_pairs_testall_cross.pkl\",\"rb\"))\n",
    "t1, t2 = zip(*testall_embs)\n",
    "testall_args1 = np.array(t1)\n",
    "testall_args2 = np.array(t2)\n",
    "testall_args1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"argmining19_simple-nn-model_cross.hdf5\")\n",
    "testall_output_pred = model.predict([testall_args1, testall_args2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk\n",
    "with open(\"predicted_labels_simple-nn_cross.csv\", \"w\") as f:\n",
    "    for i, a in enumerate(testall_output_pred):\n",
    "        f.write(str(cross_test_all.index.values[i]) + \",\" + str(a[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
